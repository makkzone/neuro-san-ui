/*
Configuration settings and constants for LLM model node configuration popup
 */

type LlmParameterType = boolean|number|string|[]

export enum ParamType {
    BOOLEAN,
    STRING,
    INT,
    FLOAT,
    ENUM
}

enum LlmModel {
    "Azure GPT 4" = "azure-gpt-4",
    "Azure GPT 3.5 turbo" = "azure-gpt-3.5-turbo",
    "Bring your own model" = "byom",
    "Google Chirp" = "google_chirp",
    "Google Codey" = "google_codey",
    "Google DeepMind Chinchilla" = "google_deepmind_chinchilla",
    "Google DeepMind Gemini (future)" = "google_deepmind_gemini",
    "Google Imagen" = "google_imagen",
    "Google MedPALM 2" = "google_medpalm_2",
    "Google PaLM 2 Bison" = "google_palm_2_bison",
    "Google PaLM 2 Gecko" = "google_palm_2_gecko",
    "Google PaLM 2 Otter" = "google_palm_2_otter",
    "Google PaLM 2 Unicorn" = "google_palm_2_unicorn",
    "Google PaLM-540B" = "google_palm_540b",
    "Google Sec-PaLM" = "google_sec_palm",
    "GPT 4" = "gpt-4",
    "GPT 3.5 turbo" = "gpt-3.5-turbo",
    "Vicuna" = "Vicuna"
}

enum TokenEncoding {
    /* eslint-disable @typescript-eslint/no-duplicate-enum-values */

    // There are dupe values in here since it's a many-to-one mapping. We may do something with the various keys
    // (model names) later.

    //  chat
    "gpt-4" = "cl100k_base",
    "gpt-3.5-turbo" = "cl100k_base",

    //  text
    "text-davinci-003" = "p50k_base",
    "text-davinci-002" = "p50k_base",
    "text-davinci-001" = "r50k_base",
    "text-curie-001" = "r50k_base",
    "text-babbage-001" = "r50k_base",
    "text-ada-001" = "r50k_base",
    "davinci" = "r50k_base",
    "curie" = "r50k_base",
    "babbage" = "r50k_base",
    "ada" = "r50k_base",

    //  code
    "code-davinci-002" = "p50k_base",
    "code-davinci-001" = "p50k_base",
    "code-cushman-002" = "p50k_base",
    "code-cushman-001" = "p50k_base",
    "davinci-codex" = "p50k_base",
    "cushman-codex" = "p50k_base",

    //  edit
    "text-davinci-edit-001" = "p50k_edit",
    "code-davinci-edit-001" = "p50k_edit",

    //  embeddings
    "text-embedding-ada-002" = "cl100k_base",

    //  old embeddings
    "text-similarity-davinci-001" = "r50k_base",
    "text-similarity-curie-001" = "r50k_base",
    "text-similarity-babbage-001" = "r50k_base",
    "text-similarity-ada-001" = "r50k_base",
    "text-search-davinci-doc-001" = "r50k_base",
    "text-search-curie-doc-001" = "r50k_base",
    "text-search-babbage-doc-001" = "r50k_base",
    "text-search-ada-doc-001" = "r50k_base",
    "code-search-babbage-code-001" = "r50k_base",
    "code-search-ada-code-001" = "r50k_base",

    //  open source
    "gpt2" = "gpt2",

    /* eslint-enable @typescript-eslint/no-duplicate-enum-values */
}

enum PromptTemplate {
    "Repair data" = "Repair data",
    "Augment data" = "Augment data",
    "Augment and repair data" = "Augment and repair data"
}

interface LlmParamField {
    description: string,

    // Data type of the parameter
    type: ParamType,

    // List of all available values. Only used for Enum types.
    allValues?: string[]

    // Default value for the field
    default_value: LlmParameterType

    // Value is an optional field that can be used within a form
    // etc to denote user input
    value?: LlmParameterType

    min?: number,
    max?: number,
    step?: number,

    isAdvanced: boolean,

    rows?: number
}

export interface LlmModelParams {
    [key: string]: LlmParamField
}

/**
 * Configuration params for data confabulation LLM
 */
export const LLM_MODEL_PARAMS_DATA_LLM: LlmModelParams = {
    "model": {
        default_value: LlmModel["GPT 3.5 turbo"].valueOf(),
        description: "Large Language Model (LLM) to be used",
        type: ParamType.ENUM,
        allValues: Object.values(LlmModel).filter((v) => isNaN(Number(v))).map(v => String(v)),
        isAdvanced: false
    },
    "temperature": {
        default_value: 0.7,
        description:
            "The temperature controls how much randomness is in the output. In general, the lower the temperature, " +
            "the more likely the LLM model will choose words with a higher probability of occurrence",
        type: ParamType.FLOAT,
        allValues: Object.values(LlmModel).filter((v) => isNaN(Number(v))).map(v => String(v)),
        min: 0.0,
        max: 1.0,
        step: 0.1,
        isAdvanced: false
    },
    "prompt_token_fraction": {
        default_value: 0.5,
        description: "The fraction of total tokens (not necessarily words or letters) to use for a prompt. " +
            "Each model_name has a documented number of max_tokens it can handle which is a total count " +
            "of message + response tokens which goes into the calculation",
        type: ParamType.FLOAT,
        min: 0.0,
        max: 1.0,
        step: 0.1,
        isAdvanced: true
    },
    "max_tokens": {
        default_value: 4096,
        description: "The maximum number of input tokens accepted by the LLM as input. Note that this varies by LLM " +
            "and by provider, and it is up to you to know the correct value for your model.",
        type: ParamType.INT,
        isAdvanced: true
    },
    "token_encoding": {
        default_value: TokenEncoding["gpt-3.5-turbo"].valueOf(),
        description: "tiktoken encoder name. Different for each model and it is up to you to know the correct " +
            "encoding for the model you have chosen.",
        type: ParamType.ENUM,
        // Get all values from enum and remove dupes
        allValues: [...new Set(Object.values(TokenEncoding).filter((v) => isNaN(Number(v))).map(v => String(v)))],
        isAdvanced: true
    },
    "table_ratio": {
        default_value: 1.0,
        description: "How much of the token space we expect the tables themselves to take up in the prompts " +
            "expressed as a proportion to the existing prompt token counts.  This helps apportion " +
            "rows into windows for large data sets. Default is 1.0, meaning that dataframe tokens should " +
            "take the same amount of space as prompt tokens.",
        type: ParamType.FLOAT,
        min: 0.0,
        max: 1.0,
        step: 0.1,
        isAdvanced: true
    },
    "confabulation_prompt": {
        default_value: "Good. Now create a distinct reasonable value for any missing value in the '{column_name}' " +
            "column that is marked as '{na_rep}' where the new value looks like existing data which is not missing. " +
            "Only print the resulting table with the original column names and with the original delimiter I gave you.",
        description: "Prompt sent to the LLM to instruct it to perform data confabulation",
        type: ParamType.STRING,
        rows: 4,
        isAdvanced: true
    },
    "reasoning_prompt": {
        default_value: "Based on the existing values in the '{column_name}' column, how would you go about creating " +
            "a distinct reasonable value for any missing value in the '{column_name}' column that is marked as " +
            "'{na_rep}' where the new value looks like existing data which is not missing?",
        description: "An optional string substitution for the default reasoning prompt",
        type: ParamType.STRING,
        rows: 4,
        isAdvanced: true
    }
}

export const LLM_MODEL_PARAMS2: LlmModelParams = {
    "model": {
        default_value: LlmModel["GPT 3.5 turbo"].valueOf(),
        description: "Large Language Model (LLM) to be used",
        type: ParamType.ENUM,
        allValues: Object.values(LlmModel).filter((v) => isNaN(Number(v))).map(v => String(v)),
        isAdvanced: false
    },
    "temperature": {
        default_value: 0.2,
        description:
            "The temperature controls how much randomness is in the output. In general, the lower the temperature, " +
            "the more likely GPT-3 will choose words with a higher probability of occurrence",
        type: ParamType.FLOAT,
        allValues: Object.values(LlmModel).filter((v) => isNaN(Number(v))).map(v => String(v)),
        isAdvanced: false
    },
    "prompt_template": {
        default_value: PromptTemplate["Repair data"].valueOf(),
        description: "Choose a pre-created template or write your own prompt",
        type: ParamType.ENUM,
        allValues: Object.values(PromptTemplate).filter((v) => isNaN(Number(v))).map(v => String(v)),
        isAdvanced: false
    },

    "system_prompt": {
        default_value:
`In the table below, change the value of the ESP TYPE field to ‘Outcome’ if the attribute is a decision objective 
that is affected by the choice of actions, and change it to ‘Action’ an action that can be taken 
in order to optimize outcomes.
 
Also, using the DB API, analyze the data and provide a sensitivity analysis.
 
Finally, given the data description is <field1>, devise some analysis on the data and provide any interesting patterns 
or insights you observe, along with charts baking your observations.`,
        description: "System prompt for the LLM",
        type: ParamType.STRING,
        isAdvanced: false
    }
}

export const LLM_MODEL_PARAMS3: LlmModelParams = {
    "model": {
        default_value: LlmModel["GPT 3.5 turbo"].valueOf(),
        description: "Large Language Model (LLM) to be used",
        type: ParamType.ENUM,
        allValues: Object.values(LlmModel).filter((v) => isNaN(Number(v))).map(v => String(v)),
        isAdvanced: false
    },

    "temperature": {
        default_value: 0.2,
        description:
`The temperature controls how much randomness is in the output. In general, the lower the temperature, the more ` +
`likely the LLM model will choose words with a higher probability of occurrence`,
        type: ParamType.FLOAT,
        allValues: Object.values(LlmModel).filter((v) => isNaN(Number(v))).map(v => String(v)),
        isAdvanced: false
    },

    "prompt_template": {
        default_value: PromptTemplate["Repair data"].valueOf(),
        description: "Choose a pre-created template or write your own prompt",
        type: ParamType.ENUM,
        allValues: Object.values(PromptTemplate).filter((v) => isNaN(Number(v))).map(v => String(v)),
        isAdvanced: false
    },

    "system_prompt": {
        default_value:
`For data sample <field1>, given the actions <field2> generated by the Prescriptor, use the API call <field3>, ` +
`with description <field4> to implement the action. Classify the result of the API call above as to adherence to 
Responsible AI policies <field5>.`,
        description: "System prompt for the LLM",
        type: ParamType.STRING,
        isAdvanced: false
    }
}
